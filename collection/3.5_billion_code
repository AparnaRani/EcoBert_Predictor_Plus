# --- 1. Install necessary packages ---
print("Installing packages (including bitsandbytes and peft for QLoRA)...")
!pip install eco2ai transformers datasets accelerate scikit-learn pandas peft bitsandbytes -q
print("Installation complete.")

# --- 2. Import Libraries ---
import os
import uuid
import pandas as pd
import torch
import eco2ai
import logging
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    BitsAndBytesConfig # For 4-bit Quantization
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType # For LoRA
import traceback
import numpy as np 
import time 

# --- 3. Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 4. Define File Paths ---
# We will append to our main CSV files
METADATA_PATH = '/kaggle/working/training_metadata.csv'
EMISSIONS_PATH = '/kaggle/working/emissions.csv'
CHECKPOINT_DIR = '/kaggle/working/results/' 
os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)
os.makedirs(os.path.dirname(EMISSIONS_PATH), exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)


# --- 4.5. Define GPU and Model Parameters ---
def get_gpu_type():
    """Detects and returns the name of the available GPU."""
    if not torch.cuda.is_available():
        logger.warning("!!! WARNING: No GPU detected. Experiments will be slow. !!!")
        return "cpu"
    gpu_name = torch.cuda.get_device_name(0)
    logger.info(f"--- GPU Detected: {gpu_name} ---")
    if "P100" in gpu_name: return "Tesla P100-PCIE-16GB"
    elif "T4" in gpu_name: return "Tesla T4"
    else: return gpu_name

# Parameter count for microsoft/phi-3-mini-4k-instruct
PHI_3_MINI_PARAMS = 3821811712 # 3.8B params

# --- 5. Main Experiment Function (with QLoRA) ---
def run_single_experiment(params: dict):
    """
    Runs a single QLoRA experiment and appends results to common CSV files.
    Ensures emissions are saved even if stopped/failed via finally block.
    """
    experiment_id = str(uuid.uuid4())
    metadata = params.copy()
    metadata['experiment_id'] = experiment_id
    metadata['gpu_type'] = get_gpu_type() 
    metadata['num_gpus'] = torch.cuda.device_count() if torch.cuda.is_available() else 0

    print(f"\n--- Starting Experiment ID: {experiment_id} ---")
    print(f"Parameters: {metadata}")

    # --- Append Metadata ---
    # This saves the parameters *before* the run starts
    all_columns = [
        'model_name', 'dataset_name', 'num_train_samples', 'num_epochs', 'batch_size',
        'fp16', 'pue', 'experiment_id', 'gpu_type', 'learning_rate', 'max_sequence_length',
        'gradient_accumulation_steps', 'num_gpus', 'dataset_config', 'model_parameters'
    ]
    metadata_df = pd.DataFrame([metadata])
    for col in all_columns:
        if col not in metadata_df.columns:
            metadata_df[col] = pd.NA
    metadata_df = metadata_df[all_columns] 
    
    try:
        # Check if file exists to decide on writing header
        file_exists = os.path.exists(METADATA_PATH)
        metadata_df.to_csv(METADATA_PATH, mode='a', header=not file_exists, index=False)
        logger.info(f"Successfully appended metadata to {METADATA_PATH}")
    except Exception as e:
        logger.error(f"CRITICAL: FAILED to save metadata. Aborting. Error: {e}")
        return 

    # --- Initialize eco2ai tracker ---
    tracker = eco2ai.Tracker(
        project_name="EcoBERT_Predictor_Data_Collection",
        experiment_description=f"run_{experiment_id}",
        file_name=EMISSIONS_PATH, 
        pue=metadata.get('pue', 1.58)
    )

    # --- Wrap Training in try...finally ---
    model = None 
    trainer = None
    tokenizer = None
    tokenized_dataset = None
    dataset = None
    data_collator = None
    
    try:
        tracker.start()
        logger.info(f"eco2ai tracker started. Appending to: {EMISSIONS_PATH}")
        
        # --- <<< QLoRA CONFIGURATION >>> ---
        # 1. Define 4-bit Quantization
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for compute
        )
        
        model_name = metadata['model_name']
        
        # 2. Load Tokenizer
        # trust_remote_code=True is required for Phi-3
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token 
            logger.info("Set tokenizer.pad_token = tokenizer.eos_token")

        # 3. Load 4-bit Model
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2,
            quantization_config=quantization_config, # Apply 4-bit
            trust_remote_code=True,
            device_map="auto", # Automatically maps model across all available GPUs (T4 x2)
            pad_token_id=tokenizer.pad_token_id
        )
        logger.info(f"Loaded {model_name} in 4-bit (QLoRA) mode.")

        # 4. Define LoRA adapters
        lora_config = LoraConfig(
            r=16, 
            lora_alpha=32,
            target_modules=["qkv_proj", "o_proj", "gate_up_proj", "down_proj"], # Specific to Phi-3
            lora_dropout=0.05, 
            bias="none", 
            task_type=TaskType.SEQ_CLS
        )
        model = get_peft_model(model, lora_config)
        logger.info("LoRA (PEFT) model activated.")
        model.print_trainable_parameters()
        # --- <<< End of QLoRA Configuration >>> ---

        # --- Dataset Processing ---
        dataset_name = metadata['dataset_name']
        num_samples = metadata['num_train_samples']
        dataset = load_dataset(dataset_name, split=f"train[:{num_samples}]")
        text_column = 'text'
        
        def tokenize_function(examples):
            return tokenizer(examples[text_column], padding=False, truncation=True, max_length=metadata['max_sequence_length'])

        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[text_column])
        tokenized_dataset.set_format("torch")
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

        # --- Training Arguments ---
        training_args = TrainingArguments(
            output_dir=os.path.join(CHECKPOINT_DIR, experiment_id),
            num_train_epochs=metadata['num_epochs'],
            per_device_train_batch_size=metadata['batch_size'],
            gradient_accumulation_steps=metadata['gradient_accumulation_steps'],
            learning_rate=metadata['learning_rate'],
            fp16=False, # Must be False for QLoRA
            bf16=True,  # Use bf16 for compute (supported by T4)
            logging_steps=100,
            report_to="none",   
            save_strategy="no", 
            seed=42,            
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator
        )
        
        logger.info("--- Starting Trainer.train() ---")
        train_result = trainer.train()
        logger.info(f"--- Trainer.train() Finished. Result: {train_result} ---")

    except Exception as e:
        print(f"ERROR during experiment {experiment_id}: {e}")
        traceback.print_exc() 
    finally:
        # --- STOP TRACKER ---
        logger.info(f"Executing finally block for {experiment_id}...")
        try:
            tracker.stop()
            print(f"--- Experiment {experiment_id} Finished. Data saved. ---")
        except Exception as stop_e:
            logger.error(f"CRITICAL: FAILED to stop tracker for {experiment_id}. Error: {stop_e}")

        # --- Clean up Memory ---
        logger.info("Cleaning up GPU memory...")
        try:
            del model, trainer, tokenizer, tokenized_dataset, dataset, data_collator
        except NameError:
            pass 
        torch.cuda.empty_cache()
        logger.info("GPU memory cleaned.")

# --- 6. Define the 5 HEAVY experiments for Phi-3-mini (QLoRA) ---
def get_phi3_configs(gpu_type: str) -> list[dict]:
    logger.info("Generating 5 HEAVY configurations for Phi-3-mini (QLoRA)...")
    
    base_params = {
        'model_name': 'microsoft/phi-3-mini-4k-instruct',
        'model_parameters': PHI_3_MINI_PARAMS,
        'dataset_name': 'imdb',
        'pue': 1.58,
        'dataset_config': np.nan,
        'fp16': False, # QLoRA uses bf16
        'learning_rate': 2e-4, # Standard for LoRA
        'gpu_type': gpu_type,
        'num_gpus': torch.cuda.device_count()
    }

    # <<< MODIFIED: 5 HEAVY variations with varying epochs >>>
    # We will vary num_epochs to get high-emission data
    configs = [
        # 1. Standard Run (1 Epoch, 25k samples)
        {**base_params, 'max_sequence_length': 512, 'num_train_samples': 25000, 
         'num_epochs': 1, 'batch_size': 2, 'gradient_accumulation_steps': 8}, # Eff 32
        
        # 2. More Epochs (Heavy, 20k samples)
        {**base_params, 'max_sequence_length': 256, 'num_train_samples': 20000, 
         'num_epochs': 3, 'batch_size': 4, 'gradient_accumulation_steps': 4}, # Eff 32
         
        # 3. Max Samples (Heavy, 50k samples)
        {**base_params, 'max_sequence_length': 256, 'num_train_samples': 50000, 
         'num_epochs': 1, 'batch_size': 2, 'gradient_accumulation_steps': 8}, # Eff 32
         
        # 4. More Epochs (Heavier, 25k samples)
        {**base_params, 'max_sequence_length': 128, 'num_train_samples': 25000, 
         'num_epochs': 4, 'batch_size': 8, 'gradient_accumulation_steps': 2}, # Eff 32
         
        # 5. Long Sequence + More Epochs (Very Heavy - run last)
        {**base_params, 'max_sequence_length': 512, 'num_train_samples': 20000, 
         'num_epochs': 2, 'batch_size': 2, 'gradient_accumulation_steps': 8}, # Eff 32
    ]
    
    return configs

# --- 7. Main execution ---
if __name__ == '__main__':
    # --- Create data/raw directory if it doesn't exist (needed by eco2ai) ---
    # This path is relative to /kaggle/working/
    os.makedirs('data/raw', exist_ok=True)
    logger.info("Ensured /kaggle/working/data/raw directory exists (for eco2ai).")

    if not torch.cuda.is_available():
         logger.error("!!! FATAL: No GPU detected by PyTorch. Aborting. !!!")
    else:
        gpu_type_str = get_gpu_type() 
        experiment_configs = get_phi3_configs(gpu_type=gpu_type_str) 
        total_exps = len(experiment_configs)
        logger.info(f"--- STARTING CAMPAIGN: {total_exps} HEAVY experiments for Phi-3-mini ---")

        for i, config in enumerate(experiment_configs):
            logger.info(f"--- Preparing Experiment {i+1}/{total_exps} ---")
            run_single_experiment(config)
            logger.info(f"--- Finished Experiment {i+1}/{total_exps} ---")
            time.sleep(15) 
            
        logger.info("--- FULL CAMPAIGN FINISHED ---")
        
        # --- Final check of output files ---
        logger.info(f"--- Listing contents of /kaggle/working/ ---")
        try:
            for f in os.listdir("/kaggle/working/"):
                logger.info(f"- {f}")
        except Exception as e:
            logger.error(f"Could not list output directory contents: {e}")

        print("\nAll 5 Phi-3-mini (QLoRA) experiments executed.")
        print(f"Check '/kaggle/working/training_metadata.csv' and '/kaggle/working/emissions.csv' for output.")