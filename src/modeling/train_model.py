import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor # Make sure you have xgboost installed: pip install xgboost
from lightgbm import LGBMRegressor # Make sure you have lightgbm installed: pip install lightgbm
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.neural_network import MLPRegressor # For a simple neural network option

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.model_selection import KFold, GridSearchCV
import numpy as np
import joblib
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns

# Configure logging to display informational messages
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def train_and_evaluate():
    logger.info("Starting model training and evaluation...")

    # --- ABSOLUTE PATH DEFINITIONS ---
    project_root = r'D:\EcoPredictor+' 
    processed_data_path = os.path.join(project_root, 'data', 'processed')
    models_path = os.path.join(project_root, 'models')
    # --- END ABSOLUTE PATH DEFINITIONS ---

    os.makedirs(models_path, exist_ok=True)

    # Load preprocessor and raw data splits generated by build_features.py
    try:
        preprocessor = joblib.load(os.path.join(models_path, 'preprocessor.joblib'))
        X_train_raw = pd.read_csv(os.path.join(processed_data_path, 'X_train_raw.csv'))
        X_test_raw = pd.read_csv(os.path.join(processed_data_path, 'X_test_raw.csv'))
        
        # <<< FIX 1: Load the CORRECT (transformed) target files ---
        # Your build_features.py saves 'y_train_transformed.csv', not 'y_train.csv'
        y_train = pd.read_csv(os.path.join(processed_data_path, 'y_train_transformed.csv')).values.ravel()
        y_test_transformed = pd.read_csv(os.path.join(processed_data_path, 'y_test_transformed.csv')).values.ravel()
        
        # <<< FIX 2: Load the ORIGINAL y_test for FINAL evaluation ---
        # This is for calculating the real-world error in kg, not log-scale
        y_test_original = pd.read_csv(os.path.join(processed_data_path, 'y_test_original.csv')).values.ravel()
        
        logger.info("Preprocessor and data splits loaded successfully.")
    except FileNotFoundError as e:
        logger.error(f"Error loading files: {e}. Please ensure build_features.py has been run successfully.")
        return 
    except Exception as e:
        logger.error(f"An unexpected error occurred during file loading: {e}")
        return


    # Apply the loaded preprocessor to transform the raw features
    logger.info("Applying preprocessor to training and testing data...")
    X_train = preprocessor.transform(X_train_raw)
    X_test = preprocessor.transform(X_test_raw)
    logger.info(f"Preprocessed X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")


    # --- Model Selection and Hyperparameter Tuning ---
    logger.info("\n--- Initiating Model Selection and Hyperparameter Tuning ---")

    # Define a dictionary of machine learning models to be evaluated
    models_to_evaluate = {
        'RandomForestRegressor': RandomForestRegressor(random_state=42, n_jobs=-1),
        'XGBoostRegressor': XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', eval_metric='mae'),
        'LGBMRegressor': LGBMRegressor(random_state=42, n_jobs=-1),
        'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42),
        'HuberRegressor': HuberRegressor(max_iter=1000), # Robust to outliers
        'MLPRegressor': MLPRegressor(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50), early_stopping=True, verbose=False) 
    }

    # Define hyperparameter grids for each model for GridSearchCV.
    param_grids = {
        'RandomForestRegressor': {
            'n_estimators': [50, 100, 200], 
            'max_depth': [10, 20, None], 
            'min_samples_split': [2, 5] 
        },
        'XGBoostRegressor': {
            'n_estimators': [100, 200], 
            'learning_rate': [0.05, 0.1], 
            'max_depth': [3, 5, 7], 
            'subsample': [0.8, 1.0] 
        },
        'LGBMRegressor': {
            'n_estimators': [100, 200], 
            'learning_rate': [0.05, 0.1],
            'num_leaves': [20, 31, 50], 
            'reg_alpha': [0.1, 0.5] 
        },
        'GradientBoostingRegressor': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5]
        },
        'HuberRegressor': {
             'epsilon': [1.0, 1.35, 1.5], 
             'alpha': [0.0001, 0.001] 
        },
        'MLPRegressor': {
            'hidden_layer_sizes': [(50,), (100, 50), (100,)], 
            'learning_rate_init': [0.001, 0.01], 
            'alpha': [0.0001, 0.001] 
        }
    }

    # We are training on log-scale, so MAE on log-scale is a valid CV metric
    scorer = make_scorer(mean_absolute_error, greater_is_better=False)

    best_model = None 
    best_score = float('inf') 
    best_model_name = ""

    cv = KFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models_to_evaluate.items():
        logger.info(f"Evaluating {name}...")
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grids.get(name, {}), 
            scoring=scorer,
            cv=cv,
            n_jobs=-1, 
            verbose=0 
        )
        # Train on the log-transformed data
        grid_search.fit(X_train, y_train) 

        current_mae = -grid_search.best_score_ 
        logger.info(f"   {name} - Best MAE (CV on log-scale): {current_mae:.4f}")
        logger.info(f"   {name} - Best Parameters: {grid_search.best_params_}")

        if current_mae < best_score:
            best_score = current_mae
            best_model = grid_search.best_estimator_ 
            best_model_name = name

    logger.info(f"\n--- Best Overall Model Selected: {best_model_name} (MAE on log-scale: {best_score:.4f}) ---")
    final_model = best_model

    # --- Evaluate the final best model on the unseen TEST set ---
    logger.info(f"Evaluating the final best model ({best_model_name}) on the test set...")
    
    # <<< FIX 3: Evaluate on the ORIGINAL scale for real-world metrics ---
    
    ## 1. Predict normalized log-scale
    predictions_norm = final_model.predict(X_test)

    # 2. Load normalization parameters from build_features
    y_mean = np.load(os.path.join(models_path, 'target_mean.npy'))
    y_std = np.load(os.path.join(models_path, 'target_std.npy'))

    # 3. Denormalize: reverse (x - mean)/std
    predictions_log = predictions_norm * y_std + y_mean

    # 4. Inverse log transform (log1p -> expm1)
    predictions_original = np.expm1(predictions_log)

    # 5. Handle any negatives (just to be safe)
    predictions_original[predictions_original < 0] = 0

    # 4. Calculate metrics by comparing to y_test_original
    rmse = np.sqrt(mean_squared_error(y_test_original, predictions_original))
    mae = mean_absolute_error(y_test_original, predictions_original)
    r2 = r2_score(y_test_original, predictions_original)

    logger.info("\n--- Final Model Evaluation Results (on Test Set - Original Scale) ---")
    logger.info(f"Selected Model: {best_model_name}")
    logger.info(f"Root Mean Squared Error (RMSE): {rmse:.4f} kg CO2")
    logger.info(f"Mean Absolute Error (MAE):       {mae:.4f} kg CO2")
    logger.info(f"R-squared (R2):                {r2:.4f}")
    logger.info("---------------------------------")

    model_save_path = os.path.join(models_path, 'emission_predictor_model.joblib')
    joblib.dump(final_model, model_save_path)
    logger.info(f"Best trained model ({best_model_name}) saved successfully to {model_save_path}.")

    # --- Visualize Predictions and Residuals (on Original Scale) ---
    logger.info("Generating prediction visualization...")
    plt.figure(figsize=(14, 6))

    # Plot 1: Actual vs. Predicted values (Original Scale)
    plt.subplot(1, 2, 1)
    # <<< FIX: Use y_test_original and predictions_original >>>
    sns.scatterplot(x=y_test_original, y=predictions_original, alpha=0.6)
    plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2, label='Perfect Prediction')
    plt.xlabel("Actual CO2 Emissions (kg)", fontsize=12)
    plt.ylabel("Predicted CO2 Emissions (kg)", fontsize=12)
    plt.title(f"Actual vs. Predicted CO2 Emissions (Test Set)\nModel: {best_model_name}", fontsize=14)
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.7)

    # Plot 2: Distribution of Residuals (Original Scale)
    plt.subplot(1, 2, 2)
    # <<< FIX: Use y_test_original and predictions_original >>>
    residuals = y_test_original - predictions_original
    sns.histplot(residuals, kde=True, bins=30, color='skyblue')
    plt.axvline(0, color='red', linestyle='--', label='Zero Residual')
    plt.xlabel("Residuals (Actual - Predicted) (kg)", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Distribution of Residuals\nModel: {best_model_name}", fontsize=14)
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.tight_layout() 
    plot_save_path = os.path.join(models_path, 'test_predictions_plot.png')
    plt.savefig(plot_save_path) 
    logger.info(f"Prediction plot saved to {plot_save_path}.")
    # plt.show() # Commented out for non-interactive runs

if __name__ == "__main__":
    train_and_evaluate()
