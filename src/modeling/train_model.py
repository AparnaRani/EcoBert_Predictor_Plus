import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor # Make sure you have xgboost installed: pip install xgboost
from lightgbm import LGBMRegressor # Make sure you have lightgbm installed: pip install lightgbm
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.neural_network import MLPRegressor # For a simple neural network option

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.model_selection import KFold, GridSearchCV
import numpy as np
import joblib
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns

# Configure logging to display informational messages
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def train_and_evaluate():
    logger.info("Starting model training and evaluation...")

    # --- ABSOLUTE PATH DEFINITIONS ---
    # IMPORTANT: Set this to the actual root path of your 'EcoPredictor+' project
    project_root = r'D:\EcoPredictor+' 
    
    # Construct paths for processed data and models directories
    processed_data_path = os.path.join(project_root, 'data', 'processed')
    models_path = os.path.join(project_root, 'models')
    # --- END ABSOLUTE PATH DEFINITIONS ---

    # Ensure the models directory exists where we'll save artifacts
    os.makedirs(models_path, exist_ok=True)

    # Load preprocessor and raw data splits generated by build_features.py
    try:
        preprocessor = joblib.load(os.path.join(models_path, 'preprocessor.joblib'))
        X_train_raw = pd.read_csv(os.path.join(processed_data_path, 'X_train_raw.csv'))
        X_test_raw = pd.read_csv(os.path.join(processed_data_path, 'X_test_raw.csv'))
        y_train = pd.read_csv(os.path.join(processed_data_path, 'y_train.csv')).values.ravel()
        y_test = pd.read_csv(os.path.join(processed_data_path, 'y_test.csv')).values.ravel()
        logger.info("Preprocessor and raw data splits loaded successfully.")
    except FileNotFoundError as e:
        logger.error(f"Error loading files: {e}. Please ensure build_features.py has been run successfully and paths are correct.")
        return # Exit if files are not found
    except Exception as e:
        logger.error(f"An unexpected error occurred during file loading: {e}")
        return # Exit for other loading errors


    # Apply the loaded preprocessor to transform the raw features
    logger.info("Applying preprocessor to training and testing data...")
    X_train = preprocessor.transform(X_train_raw)
    X_test = preprocessor.transform(X_test_raw)
    logger.info(f"Preprocessed X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")


    # --- Model Selection and Hyperparameter Tuning ---
    logger.info("\n--- Initiating Model Selection and Hyperparameter Tuning ---")

    # Define a dictionary of machine learning models to be evaluated
    models_to_evaluate = {
        'RandomForestRegressor': RandomForestRegressor(random_state=42, n_jobs=-1),
        'XGBoostRegressor': XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', eval_metric='mae'),
        'LGBMRegressor': LGBMRegressor(random_state=42, n_jobs=-1),
        'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42),
        'HuberRegressor': HuberRegressor(max_iter=1000), # Robust to outliers
        'MLPRegressor': MLPRegressor(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50), early_stopping=True, verbose=False) 
    }

    # Define hyperparameter grids for each model for GridSearchCV.
    # GridSearchCV will explore these parameter combinations.
    param_grids = {
        'RandomForestRegressor': {
            'n_estimators': [50, 100, 200], # Number of trees in the forest
            'max_depth': [10, 20, None], # Maximum depth of the tree (None means unlimited)
            'min_samples_split': [2, 5] # Minimum number of samples required to split an internal node
        },
        'XGBoostRegressor': {
            'n_estimators': [100, 200], # Number of boosting rounds
            'learning_rate': [0.05, 0.1], # Step size shrinkage to prevent overfitting
            'max_depth': [3, 5, 7], # Maximum depth of a tree
            'subsample': [0.8, 1.0] # Subsample ratio of the training instance
        },
        'LGBMRegressor': {
            'n_estimators': [100, 200], # Number of boosting rounds
            'learning_rate': [0.05, 0.1],
            'num_leaves': [20, 31, 50], # Max number of leaves in one tree
            'reg_alpha': [0.1, 0.5] # L1 regularization
        },
        'GradientBoostingRegressor': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5]
        },
        'HuberRegressor': {
             'epsilon': [1.0, 1.35, 1.5], # The parameter which determines the threshold at which the difference between the prediction and the target is considered an outlier.
             'alpha': [0.0001, 0.001] # The regularization parameter.
        },
        'MLPRegressor': {
            'hidden_layer_sizes': [(50,), (100, 50), (100,)], # Architecture of the hidden layers
            'learning_rate_init': [0.001, 0.01], # Initial learning rate
            'alpha': [0.0001, 0.001] # L2 penalty (regularization term) parameter.
        }
    }

    # Define the scoring metric for GridSearchCV. We use Mean Absolute Error (MAE) and set greater_is_better=False
    # because GridSearchCV maximizes scores, and we want to minimize MAE.
    scorer = make_scorer(mean_absolute_error, greater_is_better=False)

    best_model = None       # To store the best model found
    best_score = float('inf') # To track the lowest (best) MAE score
    best_model_name = ""    # To store the name of the best model

    # KFold for cross-validation ensures robust evaluation
    cv = KFold(n_splits=5, shuffle=True, random_state=42)

    # Iterate through each model, perform GridSearchCV, and compare results
    for name, model in models_to_evaluate.items():
        logger.info(f"Evaluating {name}...")
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grids.get(name, {}), # Use the grid for the current model, or empty dict if none
            scoring=scorer,
            cv=cv,
            n_jobs=-1, # Use all available CPU cores for parallel processing
            verbose=0  # Set to 1 or 2 for more verbose output during grid search
        )
        grid_search.fit(X_train, y_train) # Fit GridSearchCV on the training data

        # The best_score_ from GridSearchCV is negated because we passed MAE with greater_is_better=False
        current_mae = -grid_search.best_score_ 

        logger.info(f"  {name} - Best MAE (CV): {current_mae:.4f}")
        logger.info(f"  {name} - Best Parameters: {grid_search.best_params_}")

        # Update best model if the current one has a lower (better) MAE
        if current_mae < best_score:
            best_score = current_mae
            best_model = grid_search.best_estimator_ # Store the best estimator (model with best params)
            best_model_name = name

    logger.info(f"\n--- Best Overall Model Selected: {best_model_name} (MAE: {best_score:.4f}) ---")
    final_model = best_model # Assign the best found model as the final model

    # --- Evaluate the final best model on the unseen TEST set ---
    logger.info(f"Evaluating the final best model ({best_model_name}) on the test set...")
    predictions = final_model.predict(X_test) # Make predictions on the test set

    # Calculate key regression metrics
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)

    logger.info("\n--- Final Model Evaluation Results (on Test Set) ---")
    logger.info(f"Selected Model: {best_model_name}")
    logger.info(f"Root Mean Squared Error (RMSE): {rmse:.4f} kg CO2")
    logger.info(f"Mean Absolute Error (MAE):      {mae:.4f} kg CO2")
    logger.info(f"R-squared (R2):                 {r2:.4f}")
    logger.info("---------------------------------")

    # Save the best trained model to the models directory
    model_save_path = os.path.join(models_path, 'emission_predictor_model.joblib')
    joblib.dump(final_model, model_save_path)
    logger.info(f"Best trained model ({best_model_name}) saved successfully to {model_save_path}.")

    # --- Visualize Predictions and Residuals ---
    logger.info("Generating prediction visualization...")
    plt.figure(figsize=(14, 6))

    # Plot 1: Actual vs. Predicted values
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=y_test, y=predictions, alpha=0.6)
    # Add a perfect prediction line (y=x) for reference
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')
    plt.xlabel("Actual CO2 Emissions (kg)", fontsize=12)
    plt.ylabel("Predicted CO2 Emissions (kg)", fontsize=12)
    plt.title(f"Actual vs. Predicted CO2 Emissions (Test Set)\nModel: {best_model_name}", fontsize=14)
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.7)

    # Plot 2: Distribution of Residuals
    plt.subplot(1, 2, 2)
    residuals = y_test - predictions
    sns.histplot(residuals, kde=True, bins=30, color='skyblue')
    plt.axvline(0, color='red', linestyle='--', label='Zero Residual') # Line at zero for ideal residuals
    plt.xlabel("Residuals (Actual - Predicted)", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Distribution of Residuals\nModel: {best_model_name}", fontsize=14)
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.tight_layout() # Adjust subplot parameters for a tight layout
    plot_save_path = os.path.join(models_path, 'test_predictions_plot.png')
    plt.savefig(plot_save_path) # Save the figure to the models directory
    logger.info(f"Prediction plot saved to {plot_save_path}.")
    plt.show() # Display the plot

if __name__ == "__main__":
    train_and_evaluate()