model_name,dataset_name,num_train_samples,num_epochs,batch_size,fp16,pue,gpu_type,learning_rate,max_sequence_length,gradient_accumulation_steps,num_gpus,model_parameters,log_model_parameters,total_tokens,compute_load,compute_log,size_cluster,gpu_power_watts,model_family,actual_CO2(kg),predicted_CO2(kg)
TinyLlama/TinyLlama-1.1B-Chat-v1.0,wikitext-2,8000,1,1,True,1.1,Tesla P100-PCIE-16GB,0.0001,224,8,1,1100000000,20.818576017659822,1792000,1971200000000000,35.21741888926775,large,250,llama,0.045846507,0.0649486186813082
gpt2-xl,imdb,5000,1,2,True,1.58,Tesla P100-PCIE-16GB,2e-05,256,1,1,1500000000,21.12873094572124,1280000,1920000000000000,35.191101580950374,large,250,gpt,7.78e-05,0.0
